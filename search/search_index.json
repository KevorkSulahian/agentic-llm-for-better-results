{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FinMAS - Financial Analysis Multi-agent System","text":"<p>This app uses LLM agents organized in a multi-agent system to analyze financial data and perform financial tasks. The app is developed during the final capstone project of the WorldQuant MSc in Financial Engineering.</p> <p>It is meant as a practical and educational app that demonstrates the state-of-the-art of LLM models applied to tasks in the financial domain, and with an extra focus on open source models and packages.</p> <p>Please visit GitHub repo for further information.</p> <p>See our Tutorial to get started.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unstructured data analysis: Analyze unstructured data such as SEC filings and News articles together with fundamental company data.</li> <li>Transparent: Get insight into token usage, performance and the data fed to the system to gain confidence in the result.</li> <li>Configurable: Adjust parameters and model selection to optimize the performance of the system.</li> <li>Ticker: Select a major ticker listed on NASDAQ or NYSE for analysis.</li> <li>Multiple LLMs supported: Use LLM from an hosted provider such as Groq or HuggingFace, or OpenAI GPT models.</li> </ul> <p>The following screenshots illustrate a news analysis crew output and the main dashboard.</p>"},{"location":"#news-analysis","title":"News analysis","text":""},{"location":"#main-dashboard","title":"Main dashboard","text":""},{"location":"configuration/","title":"Configuration","text":"<p>The default parameters of the app can be set up in the config.yaml</p> <p>Some of the most important parameters are explained here:</p> <ul> <li>Embedding model: An embedding model can be set for the news data and the SEC filing separately.</li> <li>Similarity Top K: This setting determines how many of similar chunks of data are sent to the LLM to solve the task.</li> <li>LLM Provider: Mainly choose <code>groq</code> or <code>openai</code> for providing LLM model. HuggingFace support is not recommended because larger models are generally not available.</li> <li>LLM Model: The LLM to be used for the agents in the crew and the query engines. The default is <code>llama3-8b-8192</code> from <code>groq</code>.</li> <li>LLM Temperature: This controls the randomness of an LLM. 0 is more deterministic, while 1 is more random.</li> <li>LLM Max tokens: Limit for the number of tokens to be generated by the LLM.</li> <li>tickers_market_cap_exclude: The market cap categories that are excluded from the financedatabase table for picking ticker.</li> <li>news_source: Choose between Benzinga and Yahoo Finance.</li> </ul>"},{"location":"development/","title":"Development","text":"<p>The project setup is inspired by both Python for Data Science and the Learn Scientific Python project. These projects give guidelines to how to set up a research project that is reproducible and with good quality.</p> <p>Commit messages are encouraged to follow the Conventional Commits specification.</p>"},{"location":"development/#styling-and-pre-commit","title":"Styling and pre-commit","text":"<p>To maintain the code quality when committing to the repo we use pre-commit with ruff, type checking for script files and formatting of pyproject.toml file. This ensures that these code quality tools are run before any commit.</p> <p>The configuration is stored in <code>.pre-commit-config.yaml</code>, and to set up the git hook scripts simply run the following in the virtual environment:</p> <pre><code>pre-commit install\n</code></pre> <p>The pre-commits can be run on all files before committing by this command:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"components/","title":"Components","text":""},{"location":"components/#architecture","title":"Architecture","text":"<p>The app is developed in a component-based fashion where dedicated components are each responsible for a separate part of the system. This makes it easier to further extend the system if needed with additional functionality within each component. Each component has its respective folder in the codebase. The main architecture of the app consists of 4 main components:</p> <ul> <li>panel: Component that handles the user interface, all user interactions, and data visualization. The component configures the system with default settings and lets the user further customize the system.</li> <li>crews: Component that is responsible for setting up and running dedicated Multi-agent systems (crews) that either focus on a specific data source or multiple data sources. Tools that the Agents use to perform their tasks are defined here.</li> <li>data: This component focuses on fetching data from sources such as SEC filings, Benzinga News, Yahoo Finance News, or Alpha Vantage Fundamentals news.</li> <li>utils: The objective of this component is to gather helper functions that are used across the different components in a common place.</li> </ul> <pre><code>---\nconfig:\n  layout: elk\n  theme: base\n  look: classic\n---\nflowchart TD\n    P[\"fa:fa-desktop panel\"] --&gt; F\n    L[\"fa:fa-comment-dots LLMs\"] --&gt; C\n    C[\"fa:fa-users crews\"] --&gt; F[\"fa:fa-sitemap FinMAS\"]\n    FA[\"fa:fa-chart-simple Fundamentals\"] --&gt; D\n    S[\"fa:fa-file SEC Filings\"] --&gt; D\n    N[\"fa:fa-newspaper News\"] --&gt; D\n    D[\"fa:fa-database data\"] --&gt; F\n    U[\"fa:fa-wrench utils\"] --&gt; F\n\n    classDef lightBlue fill:#ADD8E6,stroke:black\n    class N,FA,L,S lightBlue\n\n    classDef strokeBlack stroke:black\n    class P,C,D,U strokeBlack\n\n    style F fill:#4CAF50,stroke:black\n</code></pre>"},{"location":"components/crews/","title":"Crew - Multi-agent System","text":"<p>The performance of a configuration of agents and tasks depend a lot on which LLM model is used for the agent executing the task. For example an Agent configuration can work very well for <code>gpt-4o-mini</code> while for <code>llama-3.2-8b</code> there is likely another configuration that will work better.</p>"},{"location":"components/crews/#agents","title":"Agents","text":"<p>TODO: Include learnings about how to define good agents</p>"},{"location":"components/crews/#tasks","title":"Tasks","text":"<p>TODO: Include learnings about how to define good task</p>"},{"location":"components/embedding_models/","title":"Embedding Models","text":"<p>For the FinMAS system to be successful the relevant data needs to be sent to the LLM agents together with the task that the LLM agents is set to perform. For the system to make a best effort to find the most relevant data for the query, it uses embedding models to convert textual data into dense numerical representations that we call embeddings. The main concept is that by storing the data as numerical vectors the model would be able to estimate which parts of the data are similar to each other and which part of the data are very different from each other.</p> <p>The user can choose from a pre-defined selection of embedding models that are retrieved from HuggingFace. When an embedding model is retrieved from HuggingFace it will be downloaded locally to the directory set in the <code>embedding_models_dir</code>. If an OpenAI model is used, then the embedding model of OpenAI will be used.</p> <p>The choice of embedding model can significantly affect the result from the analysis done by the Multi-agent system, as the model is responsible for finding the relevant data to sent to the LLM agent.</p>"},{"location":"components/embedding_models/#huggingface-embedding-models","title":"HuggingFace embedding models","text":"<p>TODO: explain the main features of the most popular models</p>"},{"location":"components/embedding_models/#openai-embedding-model","title":"OpenAI embedding model","text":"<p>When using the OpenAI embedding model ada-002 it will consume tokens during the embedding process. For large datasets, the token consumption can be somewhat high and this should be taken into consideration when using this model.</p>"},{"location":"components/llm_models/","title":"Large Language Models","text":""},{"location":"components/llm_models/#llama","title":"Llama","text":""},{"location":"components/packages/","title":"Packages","text":"<p>The following sections include a short description about key packages that are used in this project. We highlight why and how these packages are used in the project.</p>"},{"location":"components/packages/#crewai","title":"crewai","text":"<p>crewai is a multi-agent orchestration package that provides a framework for how a system (or a crew of agents) can be defined and executed. The main components of the framework are Agents, Tasks, Tools, and Crew.</p> <p>Each Agent is configured with a Role, Goal and Backstory that guides the agent to how it will approach and solve Tasks that are given to it. Tasks are defined with a Description and Desired Output.</p> <p>A essential part of an autonoumous agent is the ability to use specified tools to solve the tasks. In the FinMAS system the Agents are given tools to fetch news data and SEC filing data.</p> <p>The Agents, Tasks, and Tools are then put together in a Crew than can receive some input data and will work to solve the tasks given to them.</p>"},{"location":"components/packages/#llama-index","title":"llama-index","text":"<p>A powerful use-case for LLMs is when the LLMs are fed custom data to solve their task. By doing this it is possible to adapt an LLM to solve unique tasks that would otherwise not be possible for a general-purpose LLM that only have access to the data that it was trained on.</p> <p>llama-index is a package that provides a framework for developing RAG applications where the goal is to make it easy for the user to import custom data and transform that data into a Vector Store by using an embedding model. After the data is transformed, the framework provides capabilities for how to connect the Vector Store with LLMs so that it is possible to query or have a chat with the data.</p>"},{"location":"components/packages/#panel","title":"panel","text":"<p>Developing a user interface for any app with a lot of data and configuration can be quite time consuming. Therefore we use the panel package, which is a web app framework focused on data science applications. There are numeruous web frameworks available in Python, however, panel provides some unique features in terms of advanced data visualization and interactivity that makes it favorable to use for our project where we want to present information on multiple tabs and in an interactive manner.</p>"},{"location":"data/","title":"Data Sources","text":"<p>This is a list of the data sources that have been integrated in this app. Those data sources that require an API Key are marked in the table. Then it is necessary to set those as a environment variable which can be done by compting the <code>.env.template</code> file in the root folder and creating a <code>.env</code> file with the API keys.</p> API Key Name Type Description Yahoo Finance Price data SEC / Edgar Filings Yahoo Finance News News Free access to the latest News via RSS Benzinga News Requires registration of Alpaca account Alpha Vantage Fundamental data Limited to 25 calls per day."},{"location":"data/fundamentals/","title":"Fundamentals","text":""},{"location":"data/news/","title":"News","text":""},{"location":"data/news/#benzinga-news","title":"Benzinga News","text":"<p>This News source is accessed through the Alpaca API.</p>"},{"location":"data/sec/","title":"SEC Filings","text":"<p>In this project we have mainly focused on the 10-K (annual) and 10-Q (quarterly) SEC filings. All of the major companies that are listed on NASDAQ or NYSE are required to file these reports. These filings provide information about the historical performance of the company, and also give information about what the future may hold for the company.</p> <p>The filing information is fetched by using the edgartools package in Python, which provides a lot of convenience functions to fetch the filings from the EDGAR database. The SEC filing is downloaded locally, and subsequently parsed to extract specific sections of the filing. So that the LLM agents can focus in on the most relevant information.</p>"},{"location":"data/sec/#which-information-are-interesting-to-extract-from-the-sec-filings","title":"Which information are interesting to extract from the SEC Filings?","text":"<p>The financial statements like income statemen, balance sheet and cash flow statement are already available from other data proivders that can provide such data in a structured format.</p> <p>Therefore the focus of analysis of an SEC filing is more directed towards the unstructured text data that exists in the filings. We have focused particularly on the following sections:</p> <ul> <li>Management's Discussion and Analysis (MD&amp;A)</li> <li>Risk Factors</li> </ul> <p>In these sections there is typically information regarding management's view of the company's performance for that last period (year or quarter), and what are the key risks that the company is facing or will face in the upcoming period.</p>"},{"location":"data/sec/#forward-looking-statements","title":"Forward-looking statements","text":"<p>A key aspect of SEC Filings is that there are a lot of what is called \"forward-looking statements\" from the management in the report. These statements provide information about what are some of the expectations from management about what the future holds for the company. This can indicate what growth trajectory is planned for the company and that can be taken into consideration when analyzing the company.</p>"},{"location":"data/sec/parsing/","title":"SEC Filings Parsing","text":"<p>One of the aspects that make parsing an SEC filing challenging is that each company have a slightly different format for the filing. Therefore we have implemented a custom process to attempt to clean up the HTML content as much as is necessary to get a common format for each filing. Then we extract the headings from the Table of Contents that is always present at the start of the filing. Then the headings can be used to extract the relevant sections from the filing.</p> <p>The process is implemented in the <code>SECFilingsParser</code> class and relies heavily on the BeautifulSoup package to parse the HTML content.</p> <p>The following simplified diagram shows the overall steps for the parsing:</p> <pre><code>---\nconfig:\n  layout: elk\n  theme: base\n  look: classic\n---\nflowchart LR\n    F[\"fa:fa-file SEC Filing\"] --&gt; C[\"fa:fa-broom Clean\"]\n    C --&gt; E\n    E[\"fa:fa-list Extract TOC\"] --&gt; RF[\"fa:fa-triangle-exclamation Extract Risk Factors\"]\n    E --&gt; MDA[\"fa:fa-comment-dots Extract MD&amp;A\"]\n\n    classDef lightBlue fill:#ADD8E6,stroke:black\n    class F lightBlue\n\n    classDef strokeBlack stroke:black\n    class E,C strokeBlack\n\n    classDef green fill:#4CAF50,stroke:black\n    class MDA,RF green\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>To get familiar with how to use the FinMAS have a look at the following tutorials:</p> <ul> <li>News Analysis</li> </ul>"},{"location":"tutorial/news/","title":"Tutorial: News Analysis","text":"<ol> <li> <p>Install and start the app as described in the README of the project.</p> </li> <li> <p>The news data is not loaded when you start the app, so the checkbox     for including news data needs to be ticked.</p> </li> <li> <p>Then click the button <code>Fetch Data</code> to download news from the selected source and in the data     range that is specified. This will create a table in the main dashboard where each article     can be inspected. The content as well as the metadata of the article can be viewed.     The news articles are loaded into memory.</p> </li> <li> <p>After the news data have been loaded, it is possible to run the News Analysis crew.     Click on the Crews tab. Configure the parameters if necessary and click the <code>Run Crew</code> button.</p> </li> <li> <p>To create a crew, a Vector Store Index is created from the news articles.     This makes it possible for an LLM to get the most relevant news articles by using     an embedding model.</p> </li> <li> <p>After the Vector Store Index is created, then the Crew is initialized with their     configuration. The analysis will take some time, but when it is finished the     results are displayed in the output area. Metrics for the analysis is also displayed, and     the final result is stored in the output folder.</p> </li> </ol> <p></p>"}]}